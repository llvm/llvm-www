<!--#include virtual="../../header.incl" -->

<div class="www_sectiontitle">Ninth LLVM Performance Workshop at CGO</div>

<ul>
  <li><b>What:</b> Ninth LLVM Performance Workshop at CGO</li>
  <li><b>When:</b> Saturday, March 1st, 2025</li>
  <li><b>Where:</b> Las Vegas, Nevada, United States</li>
  <a href="https://2025.cgo.org/attending/Venue" target="_blank"> The Westin Las Vegas Hotel &amp; Spa, 160 East Flamingo Road, Las Vegas, Nevada, USA, 89109 </a> [In person]
  <li><b>Proposals should be submitted to:</b> <a href="https://easychair.org/conferences/?conf=llvmcgo2025">
    Easychair Submission</a></li>
  <li> <b>The deadline for receiving submissions is:</b> February 1st, 2025 <s>January 25th, 2025</s> </li>
  <li> <b>Speakers will be notified of acceptance or rejection by:</b> February 2nd, 2025 <s>February 1st, 2025</s> </li>
  <li> Note: Travel grants are available to eligible candidates upon request. Please reach out to the program committee if you need a travel grant. </li>
  <li> Note: Invitation letters for visa application are available upon request. Please reach out to the program committee if you need an invitation letter.</li>
</ul>

<p>
  The Ninth LLVM Performance Workshop will be held at
  (<a href="https://2025.cgo.org/track/cgo-2025-workshops-and-tutorials">CGO 2025</a>). The
  workshop is co-located with CC, HPCA, and PPoPP.
  If you are interested in attending the workshop, please register at the
  (<a href="https://conf.researchr.org/home/cgo-2025">CGO website</a>).
  The LLVM workshop at CGO will be in-person.
</p>

<p>
  Program Committee:
  <ul>
    <li>Aditya (hiraditya at msn.com)</li>
    <li>Jose M Monsalve Diaz (jmonsalvediaz at anl.gov)</li>
    <li>Shilei Tian (i at tianshilei.me)</li>
    <li>Rafael Andres Herrera Guaitero (rafaelhg at udel.edu)</li>
    <li>Kevin Sala (ksala at llnl.gov)</li>
  </ul>
</p>

<div class="www_sectiontitle" id="workshop-schedule">Schedule</div>

<table width="100%" border="1">
  <tbody>
    <tr style="font-weight: bold">
      <td style="min-width: 160px;"><p>Time (EDT)</p></td>
      <td><p>Speaker</p></td>
      <td><p>Title</p></td>
      <td><p>Topic</p></td>
    </tr>
    <!-- Opening Remarks -->
    <tr>
      <td><p>8:30 - 9:30 (60 min)</p></td>
      <td>
        <p>
          <a href="https://www.linkedin.com/in/josemonsalve2/">Jose M Monsalve Diaz</a><br />
          <a href="https://www.linkedin.com/in/shiltian/">Shilei Tian</a><br />
          <a href="https://twitter.com/_hiraditya_">Aditya</a><br />
          <a href="https://www.linkedin.com/in/randres-herrera/">Rafael A Herrera Guaitero</a><br />
          <a href="https://www.linkedin.com/in/kevinsalapenades/">Kevin Sala</a>
        </p>
      </td>
      <td><p>Opening Remarks + RISC-V Work Discussion</p></td>
      <td><p>Welcome and Introduction</p></td>
    </tr>
    <!-- Talk 1 -->
    <tr>
      <td><p>9:30 - 10:00 (30 min)</p></td>
      <td><p>Corbin Robeck, Yuanwei Fang, Keren Zhou</p></td>
      <td><p><a href="#talk1">The Proton Dialect: An MLIR Dialect For AI Compiler GPU Kernel Profiling</a></p></td>
      <td><p>Performance Analysis, Optimization, MLIR, GPU Profiling</p></td>
    </tr>
    <!-- Morning Coffee Break -->
    <tr>
      <td><p>10:00 - 10:30 (30 min)</p></td>
      <td><p>-</p></td>
      <td><p>Coffee Break</p></td>
      <td><p>-</p></td>
    </tr>
    <!-- Talk 2 -->
    <tr>
      <td><p>10:30 - 11:00 (30 min)</p></td>
      <td>
        <p>
          Rafael Andres Herrera Guaitero,<br/>
          Joseph B. Manzano Franco,<br/>
          Joshua D. Suetterlein,<br/>
          Xiaoming Li,<br/>
          Andres Marquez
        </p>
      </td>
      <td><p><a href="#talk2">CARTS: Enabling Event-Driven Task and Data Block Compilation for Distributed HPC</a>
        <br/><span style="font-family: monospace;">[<a href="slides/carts.pdf">slides</a>]</span></p></td>
      
      <td><p>HPC, Compiler, OpenMP, MLIR, ARTS</p></td>
    </tr>
    <!-- Talk 3 -->
    <tr>
      <td><p>11:00 - 11:30 (30 min)</p></td>
      <td><p>Baodi Shan, Barbara Chapman, Johannes Doerfert</p></td>
      <td><p><a href="#talk3">Fuzzlang: Leveraging Transformers and LLM Agents for Enhanced Compilation Error Repair</a>
        <br/><span style="font-family: monospace;">[<a href="slides/fuzzlang.pdf">slides</a>]</span></p></td>
      <td><p>LLVM, LLM Agents, Compilation Errors</p></td>
    </tr>
    <!-- Talk 4 -->
    <tr>
      <td><p>11:30 - 12:00 (30 min)</p></td>
      <td><p>Miguel Romero Rosas, Rudolf Eigenmann</p></td>
      <td><p><a href="#talk4">Effective Tuning of Automatically Parallelized OpenMP Applications Using Two Classical Optimizing Compilers</a>
        <br/><span style="font-family: monospace;">[<a href="slides/tuning.pdf">slides</a>]</span></p></td>
      <td><p>Performance Evaluation, Classical Optimizing Compilers, Automatic Tuning</p></td>
    </tr>
    <!-- Lunch Break -->
    <tr>
      <td><p>12:00 - 13:00 (60 min)</p></td>
      <td><p>-</p></td>
      <td><p>Lunch Break</p></td>
      <td><p>-</p></td>
    </tr>
    <!-- Talk 5 -->
    <tr>
      <td><p>13:00 - 13:30 (30 min)</p></td>
      <td><p>Yongtai Li, Chunyu Liao, Ji Qiu</p></td>
      <td><p><a href="#talk5">Comparative Analysis of Compiler Performance for RISC-V on SPEC CPU 2017</a>
        <br/><span style="font-family: monospace;">[<a href="slides/riscv_on_spec_cpu.pdf">slides</a>]</span></p></td>
      <td><p>LLVM, GCC, Compiler Optimization, RISC-V, SPEC CPU 2017, Code Size, Dynamic Instruction Count, Auto-Vectorization</p></td>
    </tr>
    <!-- Talk 6 -->
    <tr>
      <td><p>13:30 - 14:00 (30 min)</p></td>
      <td><p>Kevin Sala, Johannes Doerfert</p></td>
      <td><p><a href="#talk6">Instrumentor: Easily Customizable Code Instrumentation based on LLVM</a>
        <br/><span style="font-family: monospace;">[<a href="slides/instrumentor.pdf">slides</a>]</span></p></td>
      <td><p>Instrumentation, LLVM, Compiler</p></td>
    </tr>
    <!-- Talk 7 -->
    <tr>
      <td><p>14:00 - 14:30 (30 min)</p></td>
      <td><p>Ehsan Amiri, Rouzbeh Paktinatkeleshteri, Hao Jin, Eric Wang, Jose Nelson Amaral</p></td>
      <td>
        <p>
          <a href="#talk7">Container Class Annotations in C++ Improve the Capability of Static Analysis in MLIR</a>
          <br/><span style="font-family: monospace;">[<a href="slides/container-class-annotations.pdf">slides</a>]</span>
        </p>
      </td>
      <td><p>Performance, Data Layout Optimization, Static Analysis, C++, MLIR, LLVM IR</p></td>
    </tr>
    <!-- Talk 8 -->
    <tr>
      <td><p>14:30 - 15:00 (30 min)</p></td>
      <td><p>Mahesh Ravishankar</p></td>
      <td><p><a href="#talk8">IREE: Compiling ML Programs Using MLIR</a></p></td>
      <td><p>ML Compilers, MLIR, LLVM</p></td>
    </tr>
    <!-- Afternoon Coffee Break -->
    <tr>
      <td><p>15:00 - 15:30 (30 min)</p></td>
      <td><p>-</p></td>
      <td><p>Coffee Break</p></td>
      <td><p>-</p></td>
    </tr>
    <!-- Talk 9 -->
    <tr>
      <td><p>15:30 - 16:00 (30 min)</p></td>
      <td>
        <p>Ivan R. Ivanov, William Moses, Emil Vatai, Toshio Endo, Jens Domke, Oleksandr Zinenko</p>
      </td>
      <td><p><a href="#talk9">Polyhedral Rescheduling of GPU Kernels To Exploit Async Memory Movement</a>
        <br/><span style="font-family: monospace;">[<a href="slides/polyhedral_gpu_rescheduling.pdf">slides</a>]</span></p></td>
      
      <td><p>GPU, Polyhedral, Scheduling, MLIR</p>
    </tr>
    <!-- Talk 10 -->
    <tr>
      <td><p>16:00 - 16:30 (30 min)</p></td>
      <td><p>Pawel Radtke, Johannes Doerfert</p></td>
      <td><p><a href="#talk10">Optimizing Accelerator Memory Transfers within libomptarget</a></p></td>
      <td><p>Target Offloading, Optimizing Memory Transfers, libomptarget</p></td>
    </tr>
    <!-- Closing Remarks -->
    <tr>
      <td><p>16:30 - 16:40 (10 min)</p></td>
      <td>
        <p>
          <a href="https://www.linkedin.com/in/josemonsalve2/">Jose M Monsalve Diaz</a><br />
          <a href="https://www.linkedin.com/in/shiltian/">Shilei Tian</a><br />
          <a href="https://twitter.com/_hiraditya_">Aditya</a><br />
          <a href="https://www.linkedin.com/in/randres-herrera/">Rafael A Herrera Guaitero</a><br />
          <a href="https://www.linkedin.com/in/kevinsalapenades/">Kevin Sala</a>
        </p>
      </td>
      <td><p>Closing Remarks</p></td>
      <td><p>Getting Feedback</p></td>
    </tr>
  </tbody>
</table>

<div class="www_sectiontitle">Abstracts</div>

<!-- Abstract for Talk 1 -->
<h3 id="talk1" class="cgo-title" style="font-weight: bold;">
  The Proton Dialect: An MLIR Dialect For AI Compiler GPU Kernel Profiling
  <br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span>
</h3>
<h4>Corbin Robeck, Yuanwei Fang, Keren Zhou</h4>
<p>
Modern machine learning compilers make heavy use of MLIR to generate code optimally for complex AI operators (sophisticated matrix multiplications, flash attention, etc.). To target the latest generation of GPU accelerators with minimal user intervention and make full use of the available hardware and software features (matrix/tensor cores, warp specialization, wave priority and scheduling, loop pipelining), requires specialized compiler passes to make performance critical decisions to map the domain specific algorithms to the underlying hardware resources. This makes performance analysis and profiling tools that integrate directly into the domain-specific language's (DSL) operations critical to achieving performance comparable to handwritten kernels.
<br/>
In this talk we present the Proton Dialect: A compiler-integrated, intra-kernel, MLIR operation set for performance analysis and optimization. The dialect approach integrates profiling and performance analysis features directly into both the upper-level ML compiler language (e.g. Python) operations and the various MLIR ops and lowerings allowing operation aware passes, like loop pipelining, to handle the dialect like any other registered dialect's operations (e.g. loads, stores, control flow, etc.).
<br/>
The dialect has been upstreamed into the popular machine learning compiler Triton with implementation details described. A walkthrough is given of cross platform (Nvidia and AMD) examples of instruction scheduling optimizations made in production grade AI kernels using the dialect interleaved directly with standard (e.g. llvm, arith) and Triton DSL dialects (e.g. Triton IR, Triton AMDGPU IR, Triton Nvidia GPU IR).
</p>

<!-- Abstract for Talk 2 -->
<h3 id="talk2" class="cgo-title" style="font-weight: bold;">
  CARTS: Enabling Event-Driven Task and Data Block Compilation for Distributed HPC
  <br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span>
</h3>
<h4>Rafael Andres Herrera Guaitero, Joseph B. Manzano Franco, Joshua D. Suetterlein, Xiaoming Li, Andres Marquez</h4>
<p>
The increasing complexity and heterogeneity of high-performance computing (HPC) systems demand innovative compiler workflows. CARTS, a Compiler framework for an scalable asynchronous many task system called ARTS, addresses this need by integrating the extensibility of MLIR with the robustness of LLVM, enabling the development of task-centric compiler pipelines optimized for distributed-memory HPC environments. ARTS is a runtime developed at the Pacific Northwest National Laboratory, provides a scalable and efficient execution environment tailored for fine-grained, event-driven tasks across distributed systems. This paper introduces the ARTS dialect, designed to represent Event-Driven Tasks (EDTs) and several scalable communication/synchronization abstract primitives between them. By bridging high-level programming models like OpenMP with low-level LLVM IR, CARTS enhances both developer productivity and execution efficiency, making it a critical advancement in the field of HPC. Additionally, CARTS's ability to integrate seamlessly with existing MLIR and LLVM ecosystems demonstrates its potential for widespread adoption across HPC domains, addressing current and future challenges in system optimization.
</p>

<!-- Abstract for Talk 3 -->
<h3 id="talk3" class="cgo-title" style="font-weight: bold;">
  Fuzzlang: Leveraging Transformers and LLM Agents for Enhanced Compilation Error Repair
  <br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span>
</h3>
<h4>Baodi Shan, Barbara Chapman, Johannes Doerfert</h4>
<p>
Compilers play a pivotal role in software development, evolving alongside the increasing complexity and diversity of programming languages. However, systematic research on the generation, classification, and reproduction of compilation errors remains sparse. Compiler developers often modify error diagnostics on a best-effort basis to meet user needs and adapt to language evolution, leaving gaps in error handling.
<br/>
To address these challenges, we introduce Fuzzlang, a novel framework for generating extensive datasets of compiler errors, both in isolation and within real-world code contexts. Fuzzlang comprises the Fuzzlang Transformer, which systematically generates diverse compilation errors from existing code, and Fuzzlang Agent, which employs large language models (LLMs) to analyze and isolate complex errors from internal compiler tests. Together, Fuzzlang generates five times more independent error types than the DeepFix database and achieves 83.1% coverage of error conditions triggered by LLVM/Clang's internal testing.
<br/>
Our evaluation demonstrates that fine-tuning LLMs with the Fuzzlang dataset substantially enhances their code repair capabilities. The precision of Llama3-8B improved from 37.22% to 93.97%, and GPT-4o-mini rose from 72.29% to 96.70%. These results highlight Fuzzlang's potential as an effective tool for advancing intelligent code repair and compiler diagnostics research through comprehensive dataset generation.

</p>

<!-- Abstract for Talk 4 -->
<h3 id="talk4" class="cgo-title" style="font-weight: bold;">
  Effective Tuning of Automatically Parallelized OpenMP Applications Using Two Classical Optimizing Compilers
  <br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span>
</h3>
<h4>Miguel Romero Rosas, Rudolph Eigenmann</h4>
<p>
Automatic parallelization, still cannot achieve the required performance to be considered a true alternative to hand parallelization. However, when combined with effective tuning techniques, it provides a promising alternative to manual parallelization of sequential programs by leveraging the computational potential of modern multi-core architectures. While automatic parallelization focuses on identifying potential parallelism in code, tuning systems refine performance by optimizing efficient parallel code segments and serializing inefficient ones based on runtime metrics.
<br/>
This study investigates the performance gap between automatically and manually parallelized OpenMP applications, addressing whether this gap can be closed through compile-time solutions or if it necessitates user-interactive or dynamic approaches. We propose a novel tuning system that employs an efficient algorithm, Combined Elimination (CE), to partition and optimize program sections individually. CE demonstrates a significant advancement over existing methods by achieving equivalent performance while reducing tuning time to 57% of the closest alternative.
<br/>
Our experimental evaluation, conducted on a 16-core system, utilizes the NAS Parallel Benchmark Suite and the Polybench Suite with both the GCC and Clang compilers. Results reveal that the tuned applications consistently outperform their original serial versions and, in several cases, exceed the performance of manually parallelized implementations.
<br/>
This work stands out as one of the few approaches delivering an auto-parallelization system with guaranteed performance improvements across diverse programs, effectively eliminating the need for extensive user experimentation to achieve optimal runtimes.
</p>

<!-- Abstract for Talk 5 -->
<h3 id="talk5" class="cgo-title" style="font-weight: bold;">
  Comparative Analysis of Compiler Performance for RISC-V on SPEC CPU 2017
  <br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span>
</h3>
<h4>Yongtai Li, Chunyu Liao, Ji Qiu</h4>
<p>
This study presents a comparative analysis of LLVM and GCC compiler performance on RISC-V using SPEC CPU2017, focusing on code size and dynamic instruction count. Results show LLVM generates smaller binaries for most C/C++ workloads but lags significantly in Fortran code. GCC demonstrates superior dynamic instruction efficiency in integer workloads, while LLVM excels in floating-point auto-vectorization using RISC-V’s V-extension. A case study on 548.exchange2_r reveals LLVM’s optimization gaps, mitigated via PASS tuning.
</p>

<!-- Abstract for Talk 6 -->
<h3 id="talk6" class="cgo-title" style="font-weight: bold;">
  Instrumentor: Easily Customizable Code Instrumentation based on LLVM
  <br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span>
</h3>
<h4>Kevin Sala, Johannes Doerfert</h4>
<p>
Code instrumentation is a widely used technique for tracking applications' behavior. Common uses of code instrumentation include debugging and error diagnosis, logging of certain events, monitoring resource usage, and analyzing performance for code optimization. Typically, instrumenting a program involves modifying its original code by inserting extra code to retrieve data regarding its runtime behavior. During the execution of the instrumented program, all this data is usually collected by a runtime component (e.g., a library) for online or offline processing.
<br/>
However, although being a common technique, compilers lack a generic mechanism for instrumenting code. For instance, in the LLVM compiler infrastructure, numerous LLVM passes manually instrument LLVM IR code with different purposes. Each of these implements an ad hoc instrumentation, missing the opportunities to improve code maintainability, reduce code replication, or simplifying the effort of developing new instrumentation-based tools.
<br/>
In this talk, we will introduce the Instrumentor, an LLVM pass that allows instrumenting code in a simple and customizable way. The Instrumentor accepts a JSON file with predefined options describing which IR instructions and patterns need recording and what details are required. The Instrumentor pass then inserts function calls that a runtime component can implement to collect the forwarded information. For instance, a tool may instrument loads, stores, function calls, memory allocations, and decide which set of information expects the runtime component. The Instrumentor aims to provide a unified and simple method for instrumenting code, reducing maintainability costs and code replication, as well as paving the path for future instrumentation-based tools. Furthermore, the Instrumentor implements several elective optimizations to reduce instrumentation's overhead and may be built as a plugin to be used in other LLVM-based compilers.
<br/>
This technical presentation will cover the functionalities of the Instrumentor, which will be useful for compiler, runtime and tool developers. We will also show its versatility through several use cases, including a novel address sanitizer implemented using the new Instrumentor.
</p>

<!-- Abstract for Talk 7 -->
<h3 id="talk7" class="cgo-title" style="font-weight: bold;">
  Container Class Annotations in C++ Improve the Capability of Static Analysis in MLIR
  <br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span>
</h3>
<h4>Ehsan Amiri, Rouzbeh Paktinatkeleshteri, Hao Jin, Eric Wang, Jose Nelson Amaral</h4>
<p>
An important case that motivates using the higher-level MLIR in LLVM compilers for C++ is the recognition of standard-library containers and their functions (such as push_back(), etc.) by the compilers. Recognizing such functions enables optimizations that are difficult to implement in a lower-level representation [1,2]. This talk argues that this observation can be generalized.

Instead of using standard-library containers, some programs implement their own container classes. However, currently, C++ does not have a mechanism for the programmer to declare that a class is a container or to provide high-level semantic information about member functions and member variables of the container class. We will present examples to argue that such a mechanism would help an MLIR C++ compiler to perform more aggressive optimizations.

One example extracted from an actual workload demonstrates the hoisting of a member function of a container. Currently, this hoisting is blocked because of imprecision in the alias analysis. However, we will show that having extra information helps an analysis performed at a higher representation of the code, such as MLIR, to discover that the hoisting is a safe transformation.

More complex optimizations can also use container information to create more robust code-transformation legality analysis. One example of such an optimization was presented in the 2023 LLVM Dev meeting [3]. Since then we have discovered more general forms for this optimization. We use some examples to argue that information about container classes is required for the legality analysis of these optimizations. Introducing a way to declare container classes in the language would make the implementation of a robust legality analysis easier.

C++ has a “Container” named requirement, which is very similar to what is proposed here. It might be useful to have an attribute with a similar definition and an extra attribute that can specify common functions and variables in a container class (inserting an element, removing an element, allocating memory, etc.). In this talk we will discuss how such extensions to the language could help compiler optimizaitons.

<a href="https://www.youtube.com/watch?v=XNOPO3ogdfQ">[1]</a>
<a href="https://www.youtube.com/watch?v=3gcw-8C9UbA&t=1s">[2]</a>
<a href="https://www.youtube.com/watch?v=T7imC0udovo">[3]</a>
</p>

<!-- Abstract for Talk 8 -->
<h3 id="talk8" class="cgo-title" style="font-weight: bold;">
  IREE: Compiling ML Programs Using MLIR
  <br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span>
</h3>
<h4>Mahesh Ravishankar</h4>
<p>
IREE is an open-source compiler stack that is designed to compile programs from ML (and similar) domains. It is built from the ground up to target multiple architectures. Built in-conjunction with developments in MLIR, it is also a driver or primary user of many dialects/transformations that have been developed in MLIR. In this talk we will discuss the design of the IREE compiler, and the reasons for it. We will also highlight how dialects and transformations built in MLIR are used in IREE, what is the current state of support for ML compilation using IREE, and the challenges faced by a compiler stack like IREE in todays ML landscape.
</p>

<!-- Abstract for Talk 9 -->
<h3 id="talk9" class="cgo-title" style="font-weight: bold;">
  Polyhedral Rescheduling of GPU Kernels To Exploit Async Memory Movement
  <br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span>
</h3>
<h4>Ivan R. Ivanov, William Moses, Emil Vatai, Toshio Endo, Jens Domke, Oleksandr Zinenko</h4>
<p>
Recent trends in high performance computing show an increase in compute power, while memory movement capabilities stagnate. A way to compensate for the growing difference between the two has been to introduce new features that enable better efficiency of moving data. An example of such a feature in NVIDIA GPUs is the capability of asynchronous copies from global to shared memory (available from the Ampere architecture onward). However, even though high-performance libraries such as cutlass make use of these instructions, their usage in general purpose hand written kernels is limited due to various reasons such as portability concerns or programming difficulty. In addition, program analysis and optimization have not kept up with this introduction.
<br/>
We present a compilation flow which allows analysis and optimization of existing parallel GPU kernels in the polyhedral framework.
<br/>
In addition, we introduce a notion of optimizing with asynchronous execution in polyhedral scheduling and show that it allows us to reschedule GPU kernels to make use of async features.
<br/>
We focus specifically on the global memory to shared memory asynchronous copy capabilities of NVIDIA GPUs.
</p>

<!-- Abstract for Talk 10 -->
<h3 id="talk10" class="cgo-title" style="font-weight: bold;">
  Optimizing Accelerator Memory Transfers within libomptarget
  <br/><span style="font-size: x-small;"><a href="#workshop-schedule"> &#9650; back to schedule</a></span>
</h3>
<h4>Pawel Radtke, Johannes Doerfert</h4>
<p>
Offloading host computations to accelerators frequently incurs a substantial penalty in the form of memory transfer overhead. Although the traditional LLVM optimization pipeline offers robust static analyses, it remains unable to address optimising offload transfers directly due to architectural decisions that relocate transfer logic and metadata management to the runtime. This presentation examines both the potential and the constraints of optimizing offload memory transfers within LLVM's offloading runtime library - libomptarget, and runtime metadata produced by OpenMP offloading directives. We evaluate the scope for eliminating redundant data transfers, removing unused data segments, and thereby reducing transfer overhead based on currently available runtime information, while also considering what additional gains might be realized through enhanced metadata availability.
</p>


<div class="www_sectiontitle">Call for Speakers</div>
<p>
  We invite speakers from academia and industry to present their work on the following list of topics
  (including and not limited to:):
</p>
<ul>
  <li>Improving performance and code-size of applications built by LLVM toolchains</li>
  <li>Improving performance of LLVM's runtime libraries</li>
  <li>Improving the security of generated code</li>
  <li>Any tools or products developed by using one of the libraries in LLVM infrastructure</li>
  <li>Performance tracking over time</li>
  <li>Compiler flags, annotations and remarks to understand and improve performance</li>
  <li>Any other topic related to improving and maintaining the performance and quality of LLVM generated code</li>
</ul>

<p>
  While the primary focus of the workshop is on these topics, we welcome any submission related to the LLVM project, its sub-projects
  (clang, mlir, lldb, Polly, lld, openmp, pstl, compiler-rt, etc.), as well as their use in industry and academia.
</p>

<p>We are looking for:</p>
<ul>
  <li>keynote speakers (30-60 minutes),</li>
  <li>technical presentations (30 minutes plus questions and discussion),</li>
  <li>tutorials (30-60 minutes),</li>
  <li>panels (30-60 minutes),</li>
  <li>BOFs (30-60 minutes)</li>
</ul>

<p>
  Proposals should provide sufficient information for the review
  committee to be able to judge the quality of the submission.
  Proposals can be submitted under the form of an extended abstract, full paper, or slides.
  Accepted presentations will be presented online. The presentations will be publicly available on
  <a href="https://llvm.org/devmtg/">https://llvm.org/devmtg/</a>
</p>

<p>
  In case of any queries please reach out to the workshop organizers:
  Aditya (hiraditya at msn.com), Jose M Monsalve Diaz (jmonsalvediaz at anl.gov),
  Shilei Tian (i at tianshilei.me), Rafael (rafaelhg at udel.edu),
  or Kevin Sala (kevin.sala at bsc.es).
</p>

<h4>What types of people attend?</h4>
<ul>
  <li>Active developers of projects in the LLVM Umbrella (LLVM core, Clang, LLDB, libc++, compiler_rt, klee, lld,
    OpenMP, etc).</li>
  <li>Anyone interested in using these as part of another project.</li>
  <li>Students and Researchers.</li>
  <li>Compiler, programming language, and runtime enthusiasts.</li>
  <li>Those interested in using compiler and toolchain technology in novel and interesting ways.</li>
</ul>

<h4>Panels</h4>
Panel sessions are guided discussions about a specific topic. The panel consists of ~3 developers who discuss a topic
through prepared questions from a moderator. The audience is also given the opportunity to ask questions of the panel.

<h4>Birds of a Feather (BoF)</h4>
A BoF session, an informal meeting at conferences, where the attendees group together based on a shared interest and
carry out discussions without any pre-planned agenda.

<h4>Technical Talks</h4>
These 20-30 minute talks cover all topics from core infrastructure talks, to project's using LLVM's infrastructure.
Attendees will take away technical information that could be pertinent to their project or general interest.

<h4>Tutorials</h4>
Tutorials are 30-60 minute sessions that dive down deep into a technical topic. Expect in depth examples and
explanations.

<div class="www_sectiontitle" id="coc">Code of Conduct</div>
<p>The LLVM Foundation is dedicated to providing an inclusive and safe
  experience for everyone. We do not tolerate harassment of participants in any
  form. By registering for this event, we expect you to have read and agree to
  the <a href="http://llvm.org/docs/CodeOfConduct.html">LLVM Code of Conduct</a>.

  We also adhere to the <a href="https://conf.researchr.org/attending/cgo-2025/code-of-conduct" target="_blank">
  CGO Code of Conduct</a>.
</p>

<!-- *********************************************************************** -->
<hr>
<!--#include virtual="../../footer.incl" -->
